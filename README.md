# EACL2024-SCI-CHAT-SharedTask
The shared task for the Workshop on Simulation of Conversational Intelligence in Chat (SCI-CHAT) serves as a place to test and compare new and established research ideas in the field of open-domain dialogue and natural language processing.

# Models Details
**Baseline Models**
* **Dialogue GPT** https://huggingface.co/microsoft/DialoGPT-large?text=Hey+my+name+is+Julien%21+How+are+you%3F
* **Blenderbot** https://huggingface.co/facebook/blenderbot-400M-distill?text=Hey+my+name+is+Mariama%21+How+are+you%3F
*  **GODEL** https://huggingface.co/microsoft/GODEL-v1_1-base-seq2seq/tree/main
*  **T5** https://huggingface.co/microsoft/GODEL-v1_1-base-seq2seq/tree/main
*  **Llama-2** https://ai.meta.com/llama/
*  **GPT-3** https://openai.com/blog/gpt-3-apps

# Human Evaluation Metrics
Here are some commonly used human evaluation metrics for dialogue evaluation:
* **Fluency:** Evaluate how smoothly and naturally the conversation flows. Human evaluators assess whether the dialogue responses are grammatically correct, coherent, and free from language errors.
  
* **Coherence:** Measures the logical flow and connectivity of conversation turns. Evaluators assess whether the responses make sense in the context of the preceding dialogue and whether they follow a coherent narrative.

* **Relevance:** Assesses the relevance of dialogue responses to user queries or statements. Evaluators determine whether the responses address user input appropriately and stay on topic.

* **Engagement:** Captures the degree of user engagement during the conversation. Evaluators consider factors such as whether the dialogue is interesting, stimulating, and holds the user's attention.

* **Naturalness:** Judges how natural and human-like the dialogue system's responses sound. Evaluators assess whether the responses mimic human conversational patterns, including the use of colloquialisms, slang, and appropriate tone.
* **Empathy:** Measures the system's ability to express empathy and understanding towards the user's emotions or concerns. Evaluators gauge whether the dialogue system provides appropriate emotional support or validation.
* **Clarity:** Evaluates how clear and understandable the dialogue responses are. Assessors determine whether the responses are concise, straightforward, and free from ambiguity.
* **Information Quality:** Assesses the accuracy and informativeness of the information provided by the dialogue system. Evaluators consider whether the responses are factually correct and helpful to the user.
* **User Satisfaction:** Obtains direct feedback from users or evaluators about their overall satisfaction with the conversation. This can be measured using Likert scales or other rating systems.
* **User Experience (UX):** Focuses on the overall experience of the user during the conversation. Evaluators assess factors such as user frustration, enjoyment, and perceived value of the interaction.
* **Task Completion:** Determines whether the dialogue system successfully accomplishes the user's goals or tasks. Evaluators assess the system's ability to understand and fulfil user requests.
* **Human-likeness:** Evaluates how closely the dialogue system mimics human conversational behaviour. Assessors judge whether the responses are indistinguishable from those of a human interlocutor.
* **Novelty:** Assesses the system's ability to introduce new and interesting information or perspectives into the conversation, making it more engaging.
* **Efficiency:** Measures the system's ability to quickly provide concise and useful information, without unnecessary verbosity or delays.
* **Politeness:** Evaluates the politeness and courtesy displayed by the dialogue system in its responses to the user, including greetings, expressions of gratitude, and respectful language.
